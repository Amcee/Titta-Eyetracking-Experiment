{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f724fbe9",
   "metadata": {},
   "source": [
    "## Extract trial data\n",
    "\n",
    "Takes output pickle (.pkl) files recorded with Titta and divides them into trials using messages sent during the experiment. Will output a folder with trial data in .csv-format (one file for each trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import errno\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350aeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_data(df_et_data, df_msg, msg_onset, msg_offset):\n",
    "    ''' Extracts data from one trial associated with specific onset and offset messages.\n",
    "\n",
    "    Args:\n",
    "        df_et_data - Pandas dataframe with sample et-data (output from Titta)\n",
    "        df_msg - Pandas dataframe containing messages\n",
    "        msg_onset (str) - message sent at stimulus onset (e.g., onset_polarbear)\n",
    "        msg_offset (str) - message sent at stimulus onset (e.g., offset_polarbear)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        df - dataframe with data from one trial\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Find timestamps for data belonging to this stimulus\n",
    "    start_idx = np.where(df_msg.msg == msg_onset)[0][0]\n",
    "    stop_idx = np.where(df_msg.msg == msg_offset)[0][0]\n",
    "\n",
    "    start_time_stamp = df_msg.system_time_stamp[start_idx]\n",
    "    stop_time_stamp = df_msg.system_time_stamp[stop_idx]\n",
    "\n",
    "    # print(start_idx, stop_idx, start_time_stamp, stop_time_stamp)\n",
    "    #\n",
    "    # Cut out samples belonging to this stimulus\n",
    "    fix_idx_start = np.searchsorted(df_et_data.system_time_stamp,\n",
    "                                    start_time_stamp)\n",
    "    fix_idx_stop = np.searchsorted(df_et_data.system_time_stamp,\n",
    "                                   stop_time_stamp)\n",
    "    df_stim = df_et_data.iloc[fix_idx_start:fix_idx_stop].copy()\n",
    "\n",
    "    return df_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fa615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# header / column names\n",
    "header = ['device_time_stamp',\n",
    "         'system_time_stamp',\n",
    "         'left_gaze_point_on_display_area_x',\n",
    "         'left_gaze_point_on_display_area_y',\n",
    "         'left_gaze_point_in_user_coordinate_system_x',\n",
    "         'left_gaze_point_in_user_coordinate_system_y',\n",
    "         'left_gaze_point_in_user_coordinate_system_z',\n",
    "         'left_gaze_origin_in_trackbox_coordinate_system_x',\n",
    "         'left_gaze_origin_in_trackbox_coordinate_system_y',\n",
    "         'left_gaze_origin_in_trackbox_coordinate_system_z',\n",
    "         'left_gaze_origin_in_user_coordinate_system_x',\n",
    "         'left_gaze_origin_in_user_coordinate_system_y',\n",
    "         'left_gaze_origin_in_user_coordinate_system_z',\n",
    "         'left_pupil_diameter',\n",
    "         'left_pupil_validity',\n",
    "         'left_gaze_origin_validity',\n",
    "         'left_gaze_point_validity',\n",
    "         'right_gaze_point_on_display_area_x',\n",
    "         'right_gaze_point_on_display_area_y',\n",
    "         'right_gaze_point_in_user_coordinate_system_x',\n",
    "         'right_gaze_point_in_user_coordinate_system_y',\n",
    "         'right_gaze_point_in_user_coordinate_system_z',\n",
    "         'right_gaze_origin_in_trackbox_coordinate_system_x',\n",
    "         'right_gaze_origin_in_trackbox_coordinate_system_y',\n",
    "         'right_gaze_origin_in_trackbox_coordinate_system_z',\n",
    "         'right_gaze_origin_in_user_coordinate_system_x',\n",
    "         'right_gaze_origin_in_user_coordinate_system_y',\n",
    "         'right_gaze_origin_in_user_coordinate_system_z',\n",
    "         'right_pupil_diameter',\n",
    "         'right_pupil_validity',\n",
    "         'right_gaze_origin_validity',\n",
    "         'right_gaze_point_validity']\n",
    "\n",
    "# Read messages and et data all participants (one .pkl-file per participant)\n",
    "files = Path.cwd().glob('*.pkl')\n",
    "\n",
    "for f in files:\n",
    "\n",
    "    pid = str(f).split(os.sep)[-1][:-4]\n",
    "\n",
    "    fh = open(f, 'rb')\n",
    "    gaze_data_container = pickle.load(fh)\n",
    "    msg_container = pickle.load(fh)\n",
    "\n",
    "    # Convert to pandas dataframes\n",
    "    df = pd.DataFrame(gaze_data_container, columns=header)\n",
    "    df_msg = pd.DataFrame(msg_container, columns=['system_time_stamp', 'msg'])\n",
    "\n",
    "    # Read message for onset and offset\n",
    "    # Assumption is that messages are on the form (must be unique)\n",
    "    # 'onset_stimulusname' for the onset of a stimulus and\n",
    "    # 'offset_stimulusname'for the offset of a stimulus\n",
    "    onset = []\n",
    "    offset = []\n",
    "    for i, row in df_msg.iterrows():\n",
    "\n",
    "        if 'onset' in row.msg:\n",
    "            onset.append(row.msg)\n",
    "        if 'offset' in row.msg:\n",
    "            offset.append(row.msg)\n",
    "    trial_msg = zip(onset, offset)\n",
    "\n",
    "    # Create a folder to put the trials\n",
    "    path = Path.cwd() / 'trials' / pid\n",
    "    try:\n",
    "        path.mkdir(parents=True, exist_ok=False)\n",
    "    except FileExistsError:\n",
    "        print(\"Folder is already there\")\n",
    "    else:\n",
    "        print(\"Folder was created\")\n",
    "\n",
    "    # Extract relevant trial data and save in format required by I2MC\n",
    "    for t in trial_msg:\n",
    "        df_trial = extract_trial_data(df, df_msg, t[0], t[1])\n",
    "\n",
    "        filename = t[0].split('_')[1] + '.tsv'\n",
    "        df_trial.to_csv(str(path) + os.sep + filename, sep='\\t')\n",
    "\n",
    "        print('Trial ' + filename + \" written to folder \", path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c57394",
   "metadata": {},
   "source": [
    "## Compute data quality\n",
    "Two files will be generated:\n",
    "\n",
    "1. **data_quality_validation.csv**. This file contains data quality values (accuracy, precision, prop. data loss) collected during the validation procedure following the calibration. \n",
    "2. **data_loss_per_trial.csv**. This file contains prop. of data loss for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Sep 16 15:29:39 2022\n",
    "\n",
    "@author: Marcus\n",
    "\"\"\"\n",
    "\n",
    "# Folder where the trails should be\n",
    "trial_folder = Path.cwd() / 'trials'\n",
    "\n",
    "#%% Compute data quality from pickle\n",
    "\n",
    "# Find unique participant pickle files (one per participant)\n",
    "files = Path.cwd().glob('*.pkl')\n",
    "\n",
    "# Go through all files and compute data quality\n",
    "data_quality = []\n",
    "for fn in files:\n",
    "\n",
    "    f = open(fn, 'rb')\n",
    "    pid = fn.name.split('.')[0]\n",
    "    gaze_data_container = pickle.load(f)\n",
    "    msg_container = pickle.load(f)\n",
    "    eye_openness_data_container = pickle.load(f)\n",
    "    external_signal_container = pickle.load(f)\n",
    "    sync_data_container = pickle.load(f)\n",
    "    stream_errors_container = pickle.load(f)\n",
    "    image_data_container = pickle.load(f)\n",
    "    calibration_history = pickle.load(f)\n",
    "    system_info = pickle.load(f)\n",
    "    # settings = pickle.load(f)\n",
    "    # python_version = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Go through all calibrations and select 'used' ones.\n",
    "    # 'Used' means that it was the selected calibration\n",
    "    # (several calibration can be made and the best one can\n",
    "    # be selected)\n",
    "    # All data quality values here are recorded during the\n",
    "    # validation procedure following directly after the calibration.\n",
    "    for c in calibration_history:\n",
    "        # if this calibration was 'used'?\n",
    "        if c[-1] == 'used':\n",
    "            data_quality.append([pid] + c[:-1])\n",
    "\n",
    "    # Also compute precision and data loss from individual trials\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data_quality, columns=['pid',\n",
    "                                         'accuracy_left_eye (deg)',\n",
    "                                         'accuracy_right_eye (deg)',\n",
    "                                         'RMS_S2S_left_eye (deg)',\n",
    "                                         'RMS_S2S_right_eye (deg)',\n",
    "                                         'SD_left_eye (deg)',\n",
    "                                         'SD_right_eye (deg)',\n",
    "                                         'Prop_data_loss_left_eye',\n",
    "                                         'Prop_ data_loss_right_eye'])\n",
    "df.to_csv('data_quality_validation.csv')\n",
    "print('Data quality values written to data_quality_validation.csv')\n",
    "\n",
    "\n",
    "# %% Compute data loss per participant and trial.\n",
    "\n",
    "# A requirement to run this analysis is that data already have been divided\n",
    "# into trials (by running extract_trial_data.py). Check if that have been done\n",
    "if  not trial_folder.exists():\n",
    "    raise FileNotFoundError(\n",
    "        errno.ENOENT, os.strerror(errno.ENOENT) + '. ***OBS! You must first run extract_trial_data.py***', trial_folder)\n",
    "\n",
    "# Go through all trials and compute data loss for each trial\n",
    "data_loss_trials = []\n",
    "trials = trial_folder.rglob('*.tsv')\n",
    "for trial in trials:\n",
    "    pid = str(trial).split(os.sep)[-2]\n",
    "\n",
    "    df_trial = pd.read_csv(trial, sep='\\t')\n",
    "    trial_name = '.'.join(str(trial).split(os.sep)[-1].split('.')[:2])\n",
    "\n",
    "    # Compute precision\n",
    "    for eye in  ['left', 'right']:\n",
    "        n_samples = len(df_trial)\n",
    "        n_valid_samples = np.nansum(df_trial[eye + '_gaze_point_validity'])\n",
    "        loss = 1 - n_valid_samples / n_samples\n",
    "        data_loss_trials.append([pid, trial_name, eye, n_samples,\n",
    "                                 n_valid_samples, loss])\n",
    "\n",
    "df_trial = pd.DataFrame(data_loss_trials, columns=['pid', 'trial',\n",
    "                                                      'eye',\n",
    "                                                      'n_trial_samples',\n",
    "                                                      'n_valid_trial_samples',\n",
    "                                                      'Prop_data_loss'])\n",
    "df_trial.to_csv('data_loss_per_trial.csv')\n",
    "print('Data loss values written to data_loss_per_trial.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed15aa3",
   "metadata": {},
   "source": [
    "## Detect fixations from extracted trial data\n",
    "\n",
    "OBS! Before running the below cell, \n",
    "\n",
    " 1.  pip install the I2MC algorithm (https://github.com/dcnieho/I2MC_Python): **pip install I2MC**.\n",
    " 2.  Change the settings under '# NECESSARY VARIABLES' below to match your particular experimental setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61224890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import I2MC\n",
    "import I2MC.plot\n",
    "import import_funcs as imp\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# NECESSARY VARIABLES\n",
    "# =============================================================================\n",
    "opt = {}\n",
    "# General variables for eye-tracking data\n",
    "opt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\n",
    "opt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\n",
    "opt['missingx']     = -opt['xres']          # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\n",
    "opt['missingy']     = -opt['yres']          # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\n",
    "opt['freq']         = 600.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n",
    "\n",
    "# Variables for the calculation of visual angle\n",
    "# These values are used to calculate noise measures (RMS and BCEA) of\n",
    "# fixations. The may be left as is, but don't use the noise measures then.\n",
    "# If either or both are empty, the noise measures are provided in pixels\n",
    "# instead of degrees.\n",
    "opt['scrSz']        = [52.7, 30]    # screen size in cm\n",
    "opt['disttoscreen'] = 63.0                  # distance to screen in cm.\n",
    "\n",
    "# Folders\n",
    "# Data folder should be structured by one folder for each participant with\n",
    "# the eye-tracking data in textfiles in each folder.\n",
    "dir_path = Path.cwd()\n",
    "folders  = {}\n",
    "folders['data']   = os.path.join(dir_path,'trials')   # folder in which data is stored (each folder in folders.data is considered 1 subject)\n",
    "folders['output'] = os.path.join(dir_path,'output')         # folder for output (will use structure in folders.data for saving output)\n",
    "\n",
    "# Options of example script\n",
    "log_level    = 1    # 0: no output, 1: output from this script only, 2: provide some output on command line regarding I2MC internal progress\n",
    "do_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as png-file in output folder.\n",
    "# the figures works best for short trials (up to around 20 seconds)\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIONAL VARIABLES\n",
    "# =============================================================================\n",
    "# The settings below may be used to adopt the default settings of the\n",
    "# algorithm. Do this only if you know what you're doing.\n",
    "\n",
    "# # STEFFEN INTERPOLATION\n",
    "opt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\n",
    "opt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\n",
    "opt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n",
    "\n",
    "# # K-MEANS CLUSTERING\n",
    "opt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\n",
    "opt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\n",
    "opt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\n",
    "opt['downsamples']          = [2, 5, 10]\n",
    "opt['downsampFilter']       = True                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n",
    "\n",
    "# # FIXATION DETERMINATION\n",
    "opt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\n",
    "opt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop algorithm from eating into saccades\n",
    "opt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\n",
    "opt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\n",
    "opt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n",
    "\n",
    "# Change parameters according to the recommendations on Github\n",
    "if opt['freq'] == 120:\n",
    "    opt['downsamples']          = [2, 3, 5]\n",
    "    opt['chebyOrder']           = 7\n",
    "\n",
    "if opt['freq'] < 120:\n",
    "    opt['downsamples']          = [2, 3]\n",
    "    opt['downsampFilter']       = False\n",
    "# =============================================================================\n",
    "# SETUP directory handling\n",
    "# =============================================================================\n",
    "# Check if output directory exists, if not create it\n",
    "if not os.path.isdir(folders['output']):\n",
    "   os.mkdir(folders['output'])\n",
    "\n",
    "# Get all participant folders\n",
    "fold = list(os.walk(folders['data']))\n",
    "all_folders = [f[0] for f in fold[1:]]\n",
    "number_of_folders = len(all_folders)\n",
    "\n",
    "# Get all files\n",
    "all_files = [f[2] for f in fold[1:]]\n",
    "number_of_files = [len(f) for f in all_files]\n",
    "\n",
    "# Write the final fixation output file\n",
    "fix_file = os.path.join(folders['output'], 'allfixations.txt')\n",
    "for it in range(1,101):\n",
    "    if os.path.isfile(fix_file) and it < 100:\n",
    "        fix_file = os.path.join(folders['output'], 'allfixations_{}.txt'.format(it))\n",
    "    else:\n",
    "        if log_level>0:\n",
    "            print('Fixations will be stored to: \"{}\"'.format(fix_file))\n",
    "        break\n",
    "\n",
    "# =============================================================================\n",
    "# START ALGORITHM\n",
    "# =============================================================================\n",
    "start = time.time()\n",
    "\n",
    "for folder_idx, folder in enumerate(all_folders):\n",
    "    if log_level>0:\n",
    "        print('Processing folder {} of {}'.format(folder_idx + 1, number_of_folders))\n",
    "\n",
    "    # make output folder\n",
    "    if do_plot_data:\n",
    "        outFold = os.path.join(folders['output'], (folder.split(os.sep)[-1]))\n",
    "        if not os.path.isdir(outFold):\n",
    "           os.mkdir(outFold)\n",
    "\n",
    "    if number_of_files[folder_idx] == 0:\n",
    "        if log_level>0:\n",
    "            print('  folder is empty, continuing to next folder')\n",
    "        continue\n",
    "\n",
    "    for file_idx, file in enumerate(all_files[folder_idx]):\n",
    "        if log_level>0:\n",
    "            print('  Processing file {} of {}'.format(file_idx + 1, number_of_files[folder_idx]))\n",
    "\n",
    "        # Get current file name\n",
    "        file_name = os.path.join(folder, file)\n",
    "        ## IMPORT DATA\n",
    "        if log_level>0:\n",
    "            print('    Loading data from: {}'.format(file_name))\n",
    "        data = imp.Titta(file_name, [opt['xres'], opt['yres']])\n",
    "\n",
    "        # check whether we have data, if not, continue to next file\n",
    "        if len(data['time']) == 0:\n",
    "            if log_level>0:\n",
    "                print('    No data found in file')\n",
    "            continue\n",
    "\n",
    "        # RUN FIXATION DETECTION\n",
    "        if log_level>0:\n",
    "            print('    Running fixation classification...')\n",
    "        try:\n",
    "            fix,_,_ = I2MC.I2MC(data,opt,log_level==2,logging_offset=\"      \")\n",
    "        except Exception as e:\n",
    "            print('    Error in file {}: {}'.format(file_name, e))\n",
    "            continue\n",
    "\n",
    "        if not fix:\n",
    "            if log_level>0:\n",
    "                print('    Fixation classification did not succeed with file {}'.format(file_name))\n",
    "            continue\n",
    "\n",
    "        if fix != False:\n",
    "            ## PLOT RESULTS\n",
    "            if do_plot_data:\n",
    "                # pre-allocate name for saving file\n",
    "                save_file = os.path.join(outFold, os.path.splitext(file)[0]+'.png')\n",
    "                f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n",
    "                # save figure and close\n",
    "                if log_level>0:\n",
    "                    print('    Saving image to: ' + save_file)\n",
    "                f.savefig(save_file)\n",
    "                plt.close(f)\n",
    "\n",
    "            # Write data to file\n",
    "            fix['participant'] = folder.split(os.sep)[-1]\n",
    "            fix['trial'] = os.path.splitext(file)[0]\n",
    "            fix_df = pd.DataFrame(fix)\n",
    "            fix_df.to_csv(fix_file, mode='a', header=not os.path.exists(fix_file),\n",
    "                          na_rep='nan', sep='\\t', index=False, float_format='%.3f')\n",
    "\n",
    "print('\\n\\nI2MC took {}s to finish!'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb500e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
